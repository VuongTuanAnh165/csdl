# train_model.py
# ‚úÖ Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost ph√¢n lo·∫°i truy v·∫•n SQL nhanh/ch·∫≠m t·ª´ log ƒë√£ khai ph√°
"""
Hu·∫•n luy·ªán b·ªô ph√¢n lo·∫°i XGBoost d·ª± ƒëo√°n truy v·∫•n SQL ch·∫≠m/nhanh t·ª´ `query_log.csv`.

C√°c b∆∞·ªõc ch√≠nh:
- ƒê·ªçc v√† l·ªçc d·ªØ li·ªáu h·ª£p l·ªá; lo·∫°i outlier theo ng∆∞·ª°ng 99% v√† th√™m `exec_time_log`.
- B·ªï sung c√°c c·ªôt c√∫ ph√°p (has_...), one-hot cho `types` t·ª´ EXPLAIN.
- Chia train/test c√≥ stratify; t√≠nh `scale_pos_weight` ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng.
- Hu·∫•n luy·ªán XGBClassifier; ƒë√°nh gi√° Accuracy/F1, l∆∞u b√°o c√°o v√† bi·ªÉu ƒë·ªì (CM, ROC, FI).
- L∆∞u model (`slow_query_model.pkl`) v√† danh s√°ch feature (`model_features.pkl`).
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    f1_score, ConfusionMatrixDisplay, roc_curve, auc
)
import xgboost as xgb
import matplotlib.pyplot as plt
import joblib
import os
import sys
import numpy as np

# ==== 1. ƒê·ªçc d·ªØ li·ªáu log t·ª´ file ====
log_path = "query_log.csv"
if not os.path.exists(log_path):
    print("‚ùå Kh√¥ng t√¨m th·∫•y file query_log.csv. H√£y ch·∫°y log_queries.py tr∆∞·ªõc.")
    sys.exit(1)

df = pd.read_csv(log_path)
df = df[df['status'] == 'OK'].copy()  # L·ªçc b·ªè truy v·∫•n l·ªói

# ==== 2. Lo·∫°i b·ªè truy v·∫•n d·ªã bi·ªát (outliers) & log-transform ====
if len(df) < 50:
    print(f"‚ö†Ô∏è D·ªØ li·ªáu ch·ªâ c√≥ {len(df)} b·∫£n ghi ‚Üí qu√° √≠t ƒë·ªÉ hu·∫•n luy·ªán. H√£y b·ªï sung query.")
    sys.exit(1)

q99 = df["exec_time_sec"].quantile(0.99)
# Lo·∫°i b·ªè outlier c·ª±c l·ªõn (top 1%) ƒë·ªÉ m√¥ h√¨nh ·ªïn ƒë·ªãnh h∆°n
df = df[df["exec_time_sec"] <= q99]
# Bi·∫øn ƒë·ªïi log gi√∫p ph√¢n ph·ªëi th·ªùi gian th·ª±c thi b·ªõt l·ªách
df["exec_time_log"] = np.log1p(df["exec_time_sec"])
print(f"‚úÖ ƒê√£ lo·∫°i b·ªè c√°c truy v·∫•n c√≥ exec_time_sec > {q99:.2f}s (top 1%) v√† th√™m log-transform")

# ==== 3. Ki·ªÉm tra nh√£n m·ª•c ti√™u ====
if 'is_slow' not in df.columns or df['is_slow'].nunique() < 2:
    print("‚ö†Ô∏è C·∫ßn c·∫£ truy v·∫•n nhanh v√† ch·∫≠m. H√£y ki·ªÉm tra l·∫°i log.")
    sys.exit(1)

print("\nüìä Ph√¢n ph·ªëi nh√£n m·ª•c ti√™u (is_slow):")
print(df['is_slow'].value_counts(normalize=True))

# ==== 4. ƒê·∫∑c tr∆∞ng syntax (has_...) ====
syntax_cols = [
    'has_like', 'has_group', 'has_join',
    'has_order', 'has_limit', 'has_distinct',
    'has_function'
]
for col in syntax_cols:
    if col not in df.columns:
        df[col] = 0

# ==== 5. One-hot encoding cho c·ªôt 'types' ====
if 'types' in df.columns:
    types_dummies = df['types'].str.get_dummies(sep=',')
else:
    types_dummies = pd.DataFrame()
    print("‚ö†Ô∏è C·ªôt 'types' kh√¥ng c√≥ trong d·ªØ li·ªáu. B·ªè qua one-hot.")

# ==== 6. K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o ====
# Danh s√°ch ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o v√† √Ω nghƒ©a:
# - rows_examined: T·ªïng s·ªë d√≤ng ∆∞·ªõc l∆∞·ª£ng ƒë√£ qu√©t (t·ª´ EXPLAIN) ‚Üí ƒë·ªô n·∫∑ng truy v·∫•n.
# - uses_index: C√≥ d√πng ch·ªâ m·ª•c ·ªü b·∫•t k·ª≥ b∆∞·ªõc n√†o (1/0) ‚Üí th∆∞·ªùng gi·∫£m th·ªùi gian.
# - num_tables: S·ªë b·∫£ng join ‚Üí join nhi·ªÅu th∆∞·ªùng t·ªën k√©m h∆°n.
# - num_predicates: S·ªë ƒëi·ªÅu ki·ªán l·ªçc (WHERE/AND/OR) ‚Üí l·ªçc nhi·ªÅu c√≥ th·ªÉ n·∫∑ng (tu·ª≥ index).
# - num_subqueries: S·ªë subquery l·ªìng nhau ‚Üí subquery s√¢u th∆∞·ªùng ch·∫≠m.
# - exec_time_log: log(1+exec_time_sec) ‚Üí gi√∫p ph√¢n ph·ªëi th·ªùi gian b·ªõt l·ªách.
base_features = [
    'rows_examined', 'uses_index', 'num_tables',
    'num_predicates', 'num_subqueries', 'exec_time_log'
] + syntax_cols

X = pd.concat([df[base_features], types_dummies], axis=1)
y = df['is_slow']

# ==== 7. Train/test split ====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print("\nüìä Ph√¢n ph·ªëi nh√£n sau khi split:")
print("Train:", pd.Series(y_train).value_counts(normalize=True))
print("Test :", pd.Series(y_test).value_counts(normalize=True))

# ==== 8. T√≠nh scale_pos_weight ƒë·ªÉ x·ª≠ l√Ω imbalance ====
pos = sum(y_train == 1)
neg = sum(y_train == 0)
# T·ª∑ l·ªá m·∫´u √¢m/d∆∞∆°ng ƒë·ªÉ c√¢n b·∫±ng loss khi l·ªõp CH·∫¨M hi·∫øm
scale_pos_weight = neg / pos if pos > 0 else 1

# ==== 9. Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost ====
eval_set = [(X_train, y_train), (X_test, y_test)]

model = xgb.XGBClassifier(
    objective="binary:logistic",
    max_depth=6,
    n_estimators=300,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    eval_metric="logloss"   # ‚úÖ ƒë·∫∑t ·ªü constructor
)

print(f"‚ö° ƒêang d√πng XGBoost {xgb.__version__}")

model.fit(
    X_train, y_train,
    eval_set=eval_set,
    verbose=False
)

if hasattr(model, "best_iteration") and model.best_iteration is not None:
    print(f"‚úÖ Best iteration: {model.best_iteration}")

# ==== 10. ƒê√°nh gi√° ====
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, zero_division=0)

print("\n=== [Confusion Matrix] ===")
print(confusion_matrix(y_test, y_pred))

print("\n=== [Classification Report] ===")
report_dict = classification_report(y_test, y_pred, zero_division=0, output_dict=True)
print(classification_report(y_test, y_pred, zero_division=0))

print(f"\n‚úÖ Accuracy: {acc:.4f} | F1-score: {f1:.4f}")

# ==== 11. L∆∞u k·∫øt qu·∫£ & bi·ªÉu ƒë·ªì ====
os.makedirs("figures", exist_ok=True)

# L∆∞u classification report ra CSV
pd.DataFrame(report_dict).transpose().to_csv("figures/classification_report.csv")
print("‚úÖ ƒê√£ l∆∞u classification_report.csv")

# Confusion matrix
plt.figure(figsize=(5, 4))
disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred),
                              display_labels=["Nhanh (0)", "Ch·∫≠m (1)"])
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix - XGBoost")
plt.tight_layout()
plt.savefig("figures/confusion_matrix.png")
plt.close()

# ROC
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, lw=2, label=f"AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - XGBoost")
plt.legend(loc="lower right")
plt.savefig("figures/roc_curve.png")
plt.close()

# Feature importance
if model.get_booster().get_score():
    xgb.plot_importance(model, importance_type="gain", height=0.5, show_values=False)
    plt.title("Feature Importance - Gain")
    plt.tight_layout()
    plt.savefig("figures/feature_importance.png")
    plt.close()
else:
    print("‚ö†Ô∏è Kh√¥ng c√≥ c√¢y n√†o ƒë∆∞·ª£c x√¢y d·ª±ng ‚Üí B·ªè qua v·∫Ω Feature Importance.")

# L∆∞u m√¥ h√¨nh
joblib.dump(model, "slow_query_model.pkl")
joblib.dump(X.columns.tolist(), "model_features.pkl")
print("‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√† ƒë·∫∑c tr∆∞ng")
